<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>HardCoreAI - Weituo Hao</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
<a href="https://github.com/weituo12321">
<img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub" />
</a>
        <header id="banner" class="body">
                <h1><a href="/">HardCoreAI </a></h1>
                <nav><ul>
                    <li><a href="/category/announcement.html">Announcement</a></li>
                    <li><a href="/category/machine_learning.html">Machine_Learning</a></li>
                    <li><a href="/category/trial.html">trial</a></li>
                </ul>
                </nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/classification_6_data_preprocessing.html">Classification_6_Data_Preprocessing</a></h1>
<footer class="post-info">
        <span>Thu 05 May 2016</span>
<span>| tags: <a href="/tag/data-science.html">data science</a></span>
</footer><!-- /.post-info --><h1>Data Exploration</h1>
<p>Basic steps:<br />
1. Variable Identification<br />
2. Univariate Analysis<br />
3. Bi-variate Analysis<br />
4. Missing values treatment<br />
5. Outlier treatment<br />
6. Variable transformation<br />
7. Variable creation  </p>
<p>We may iterate over steps 4-7 multiple times before we come up wiht refined model.  </p>
<h4>Step 1: Varibale Identification</h4>
<p>In this step, we must get clear following questions: What are the input and output variables ? What the data type of each variable ? What the category of each variable ?  </p>
<p>Type of variable:<br />
Input<br />
Output  </p>
<p>Data type:<br />
Character<br />
Numeric  </p>
<p>Variable Category:<br />
Categorical<br />
Continuous  </p>
<h4>Step 2: Univariate Analysis</h4>
<p>In this step, we explore variables one by one. Methods used to exlpore the data depend on the category of the variable.  </p>
<p>2.1 For continuous variable, we need to understand the central tendency and spread of the variable.  </p>
<p>Central Tendency:<br />
Mean<br />
Median<br />
Mode<br />
Min<br />
Max  </p>
<p>Measure of Dispersion:<br />
Range<br />
Quartile<br />
Variance<br />
Standard Deviation<br />
Skewness and Kurtosis  </p>
<p>Visualization Methods:<br />
Histogram<br />
Box plot  </p>
<p>2.2 For categorical variables, we need to use frequency table to understand distribution of each category. We also read some percentage information of values under each category. Bar chart will be helpful as visualization.  </p>
<h4>Step 3: Bi-variate Analysis</h4>
<p>In this step, we find out the relationship between two variables. We can explore the association and disassociation between variables at a pre-defined significance level. The combination of variables can be categorical &amp; categorical, categorical &amp; continuous, continuous &amp; continuous.  </p>
<p>3.1 continuous &amp; continuous<br />
While doing bi-variate analysis between two continuous variables, we should look at scatter plot. The relationship can be linear or non-linear. Then the strength of relationship can be indicated by correlation. Correlation varies between -1 and +1. -1 means perfect negative linear correlation. +1 means perfect positive linear correlation. 0 means no correlation. <br />
</p>
<div class="math">$$Correlation = \frac{Covairance(X, Y)} {\sqrt{Var(X)*Var(Y)}}$$</div>
<p>3.2 categorical &amp; categorical<br />
two-way table : The rows represent the category of one variable and the columns represent the categories of the other variable. We show the count of count% of observations available in each combination of row and column categories. Or we can use stacked column chart to visualize the two-way table.  </p>
<p>chi-square test: It is used to derive the statistical significance of relationship between the variables. It returns the probability for the computed chi-square distribution with the degree of freedom.<br />
Probability 0 means both categorical variable are dependent<br />
Probability 1 means both variables are independent<br />
Probability less than 0.05 means the relationship between the variables is significant at 95% confidence. The chi-square test statistic for a test of independence of two categorical variables is found by:<br />
</p>
<div class="math">$$X^2 = \sum (O - E)^ 2 / E$$</div>
<p><br />
where \( O\) represents the observed frequency. \( E\) is the expected frequency under the bull hypothesis and computed by:  </p>
<div class="math">$$E = \frac{row_total * column_total}{sample size}$$</div>
<p>Statistical Measure used to analyze the power of relationship are:<br />
Cramer's V for Nominal Categorical Variable<br />
Mantel-Haenszed Chi-Square for ordinal categorical variable.  </p>
<p>3.3 categorical &amp; continuous:<br />
Z-test/T-test: Either test assess whether mean of two groups are statistically different from each other or not.<br />
</p>
<div class="math">$$z= \frac{|\bar{x_1} - \bar{x_2}|}{\sqrt{ \frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}$$</div>
<p><br />
If the probability of \( Z\) is small then the difference of two averages is more significant. The T-test is very similar to Z-test but it is used when number of observation for both categoires is less than 30.  </p>
<h4>Step 4: Missing Value Treatment</h4>
<p>Missing value can lead to bias of our model then wrong predicton or classification. They may be caused by data extraction and data collection. There are missing completely at random (missing variable is same for all observations), missing at random(variable is missing at random and missing ratio varies for different values or other input variables, for example, we are collectiong data for age and female has higher missing value compare to male), missing that depends on unobserved predictors and missing that depends on the missing value itself (for example, people with higher or lower income are likely to provide non-response to their earning).  </p>
<p>Four ways to trreat missing values
4.1 Deletion<br />
We have talked about the deletion ways in previous "how to handle missing data". Note that the deletion methods are used when the nature of missing data is "Missing completely at random" </p>
<p>4.2 Mean/Mode/Median Imputation<br />
Mean and Median imputation is used for quantitatie attributes and mode is used for qualitive attributes. It can be two types:<br />
Generalized Imputation: we calculate the mean or median for all non missing values.<br />
Similar Case Imputation: For example, we calculate the average earning for "Male" and "Female" separately. And use the averages to replace the missing value with the same gender.  </p>
<p>4.3 Prediction Model<br />
It is a sophiscated way to substitute missing data. In this case, we divide our data set into two sets: one set with no missing values and another with missing values. Next we create a model to predict missing attributes based on other attributes. We can use regression, ANOVA, logistic regression to perform this. But two drawbacks for this approach:<br />
1 The model estimated values are usually more well-behaved than the true values.<br />
2 If there are no relationships with attributes in the data set and the attribute with missing values, then the model will not be precise for estimating missing values.  </p>
<p>4.4 KNN Imputation<br />
The missing values of an attribute are imputed using the given number of attributes that are most similar to the attribute whose values are missing. The similarity of two attributes is determined using a distance function. 
Pros: knn can predict both quantitative and qualitative attributes, no model is required and attributes with multiple missing values can be easily treated.<br />
Cons: knn is very time-consuming and the choice k is very critical to the performance.  </p>
<h4>Step 5: Outlier Detection and Treatment</h4>
<p>Outlier can lead to wildly wrong estimations. There are two types of outliers: univariate and multivariate. Univariate is easy to understand. Outliers can be found when we look at distribution of a single variable. Multivariate outliers are outliers in an n-dimensional space. In order to find them, you have to look at distributions in multi-dimensions.For example, we have two variables height and weight. Two box plots of these variables show no outliers. But if we draw scatter plot in a 2D plane with weight and heigh as axises, we may find some outliers.  </p>
<p><img alt="im" src="/images/outlier_1.png" /></p>
<p>Outliers can be artificial or natural. They come from data entry errors, measurement error, experimental error, and so on so forth. Outliers' impact in data are: it increases the error variance and reduces the power of statistical modeling, bias or influence estimates that may be of substantive interest or basic assumption of regression and other statistical model assumpitons.  </p>
<p>Most commonly used method to detect outliers is visualization. Box-plot, Histogram and scatter plot are important ways to detect outliers. Other thumb rules can also be used like capping methods(any value which out of range of 5th and 95th percentile can be considered as outlier), data points(three or more standard deviation away from mean are considered outlier). Bivariate and multivariabte outliers are typically measured using either an index of influence or leverage, or distance. Popular indices such as Mahalanobis's distance and Cook's D are frequently used to detect outliers.  </p>
<p>To treat outliers, we have four commonly used ways.<br />
5.1 Deleting observations<br />
We delete outlier values if it is due to data entry error, data processing error or outlier observations are very small in numbers. We can also use trimming at both ends to remove outliers.  </p>
<p>5.2 Transforming and binning values<br />
Transforming variables can also eliminate outliers. For example, natural log of a value reduces the variation caused by extreme values. Binning is also a form of variable transformation. Decision tree algorithm allows to deal with outliers well due to binning of variable. We can also use the process of assigning weights to different observations.  </p>
<p>5.3 Imputing<br />
Like what we did in missing values, we can impute outliers by mean, median or mode only if the outliers are artificial. We can also use statistical model to predict values of outlier observation.  </p>
<p>5.4 Treat separately<br />
If there are significant number of outliers, we should treat them separately in the statistical model. One of the approach is to treat both groups as two different groups and build individual model for both groups and them combine the output.  </p>
<h4>Step 6: Variable Transformation</h4>
<p>We call step 6 and step 7 both as feature engineering. That's where the art comes into science. We are not adding any new data here, but making data more useful. For example, if we try to predict foot fall in a shopping mall based on dates. But foot fall is less affected by the day of the month than it is by the day of the week. After previous 5 steps, we will start feature engineering. There are two steps: variable transformation and variable creation.  </p>
<p>Variable transformation means the replacement of a variable by a function. For example we can use the logarithm of x to replace x. This replacement changes the distribution or relationship of a variable with others. A natural question is when to make such data transformation.   </p>
<p>First, when we want to change the scale of a variable or standardize the values of a variable for better understanding.  </p>
<p>Second, we can transform complex non-linear relationships into linear relationships. Log transformation is one of the commonly used transformation technique used in these situations.  </p>
<p>Third, symmetric distribution is preferred over skewed distribution as it is easier to interpret and generate inferences. Some modeling techniques requires normal distribution of variables. So, when we have a skewed distribution, we can use transformation which reduce skewness. For right skewed distribution, we take square/cube root or logarithm of variable. For left skewed, we take square / cube or exponential of variables.  </p>
<p>Fourth, variable transformation is also done from an implementation point of view. For example, binning of variables using decision tree is also a sort of variable transformation. So commonly used transformation ways are logarithm (powerful to change the shape reducing right skewness but can not be applied to zero or negative values), square/ cube root, binning.  </p>
<h4>Step 7: Variable Creation</h4>
<p>Feature / Variable creation is a process to generate new variables /features based on existing variables. For example, if we have date (dd-mm-yy) as an input variable in a data set. We can generate new variables like day, month, year, week, weekday that may have better relationship with target variable. Various techniques to create new features. Two commonly used ways are:  </p>
<p>7.1 Creating derived variables<br />
This refers to creating new variables from existing variables using set of functions or different methods. For example in Titanic-Kaggle competition we use the salutation of name as a new variable to predict missing ages.  </p>
<p>7.2 Creating dummy variables<br />
One of the most common application of dummy variable is to convert categorical variable into numerical variables. We have talked about this method in our previous articles.  </p>
<p>Always remember, the more effort on data exploration the better performance the model has. Some other tips for variable creation will be listed as below.  </p>
<p>7.3 create variables for difference in date, time and addresses<br />
For example, an applicant who takes days to fill in an application form is likely to be less interested  in the product compared to someone who fills in the same application within 30 minutes. Similarly, a customer living closer to a bank branch is more likely to have a higher engagement than a customer living far off.  </p>
<p>7.4 Create new ratios and proportions  </p>
<p>Instead of just keeping inputs and outputs, creating new ratios out of the might add a lot of value. For example, instead of absolute number of card sold, we prefer the ratios like credit card sales / sales person or credit card sales / marketing spend.  </p>
<p>7.5 Apply standard transformations<br />
Log, expoenntial, quadratic and trignometric variations creates more representable relationship.  </p>
<p>7.6 Check variables for seasonality and create the mdoel for right period<br />
A lot of businesses face some kind of seasonality. It could be driven by tax benefits, festive season or weather.  </p>
<p>We talked about the different methods and ideas to explore the data. This stage is as important as the model setting up. More approaches and ideas come from experience and practice. The intuition on observations is what we called art, which makes data scientists a hot group of people.  </p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="/classification_5_large_datasets_and_evaluation.html" rel="bookmark"
                           title="Permalink to Classification_5_Large_Datasets_and_Evaluation">Classification_5_Large_Datasets_and_Evaluation</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sat 30 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info -->                <p>We will talk about the combination of weak classifiers can make big difference in classification. We call it boosting.</p>
                <a class="readmore" href="/classification_5_large_datasets_and_evaluation.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/classification_4_boosting.html" rel="bookmark"
                           title="Permalink to Classification_4_Boosting">Classification_4_Boosting</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Thu 28 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info -->                <p>We will talk about the combination of weak classifiers can make big difference in classification. We call it boosting.</p>
                <a class="readmore" href="/classification_4_boosting.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/classification_3_decision_trees.html" rel="bookmark"
                           title="Permalink to Classification_3_Decision_Trees">Classification_3_Decision_Trees</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Wed 27 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info -->                <p>Previously we always played with models having a linear combination structure. This article will introduce a totally different model. Decision trees are so simple and so powerful that ada-boosting based on decision trees is often the key to win competitions.</p>
                <a class="readmore" href="/classification_3_decision_trees.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/classification_1_linear_classifiers.html" rel="bookmark"
                           title="Permalink to Classification_1_Linear_Classifiers">Classification_1_Linear_Classifiers</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sat 23 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info -->                <p>What is classification? In previous chapter we have talked about regression. Instead of real values, classification outputs predictions as some boolean values or labels. Huge amount of problems in machine learning are classification problems such as sentiment analysis, spam filtering, webpage analysis, image classification, personalized medical diagnosis, etc. The models we will cover in this chapter will be linear classifiers, logistic regression, decision trees, and ensemble methods. If time is allowed, support vector machines will also be included. The algorithms to solve the models are gradient descent, stochastic gradient descent, recursive greedy and boosting. Some practical issues like handling missing data, precision-recall tradeoff, and online learning will also be talked about.</p>
                <a class="readmore" href="/classification_1_linear_classifiers.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/classification_2_linear_classifiers_2.html" rel="bookmark"
                           title="Permalink to Classification_2_Linear_Classifiers_2">Classification_2_Linear_Classifiers_2</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sat 23 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info -->                <p>In previous article, we have talked about the basic linear classifiers and some practical issues. So we start from the metric of the models in terms of maximum likelihood estimation.</p>
                <a class="readmore" href="/classification_2_linear_classifiers_2.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_6_nearest-neighbor-and-kernel-regression.html" rel="bookmark"
                           title="Permalink to Regression_6_Nearest Neighbor and Kernel Regression">Regression_6_Nearest Neighbor and Kernel Regression</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sun 17 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>Previously we have talked about LASSO in which we used norm 1 as the constraints to select features ad get sparse solutions. In this article, we will continue to discuss nearest neighbor and kernel regression.</p>
                <a class="readmore" href="/regression_6_nearest-neighbor-and-kernel-regression.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_5_lasso.html" rel="bookmark"
                           title="Permalink to Regression_5_LASSO">Regression_5_LASSO</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Fri 15 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>Previously we have talked about ridge regression in which we used norm 2 as the constraints to balance the bias and variance. In this article, we will use norm 1 which is called least absolute shrinkage and selection operator and LASSO for short.</p>
                <a class="readmore" href="/regression_5_lasso.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_4_ridge_regression.html" rel="bookmark"
                           title="Permalink to Regression_4_Ridge_Regression">Regression_4_Ridge_Regression</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Fri 08 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>Previously we have talked about simple regression and for a machine learning model we want to get a tradeoff between bias and variance. Now we introduce one tuning parameter and check how ridge regression works.</p>
                <a class="readmore" href="/regression_4_ridge_regression.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_3_simple_regression.html" rel="bookmark"
                           title="Permalink to Regression_3_Simple_Regression">Regression_3_Simple_Regression</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Tue 05 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>We have generic models and algorithms to solve them. But we always have to worry about how much i am losing. If our model prediction is too low, then we only get low offers; if model prediction is too high, we get few looks and no offers. So this article will presents how to assess our models.</p>
                <a class="readmore" href="/regression_3_simple_regression.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
            </ol><!-- /#posts-list -->
<p class="paginator">
    Page 1 / 2
        <a href="/author/weituo-hao2.html">&raquo;</a>
</p>
            </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://twitter.com/VicWeituo">twitter</a></li>
                            <li><a href="https://github.com/weituo12321">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <p>Powered by <a href="http://getpelican.com/">Pelican</a>. Theme <a href="https://github.com/blueicefield/pelican-blueidea/">blueidea</a>, inspired by the default theme.</p>
        </footer><!-- /#contentinfo -->

</body>
</html>