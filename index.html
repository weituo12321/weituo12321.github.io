<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>HardCoreAI</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
<a href="https://github.com/weituo12321">
<img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub" />
</a>
        <header id="banner" class="body">
                <h1><a href="/">HardCoreAI </a></h1>
                <nav><ul>
                    <li><a href="/category/announcement.html">Announcement</a></li>
                    <li><a href="/category/machine_learning.html">Machine_Learning</a></li>
                    <li><a href="/category/trial.html">trial</a></li>
                </ul>
                </nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/regression_6_nearest-neighbor-and-kernel-regression.html">Regression_6_Nearest Neighbor and Kernel Regression</a></h1>
<footer class="post-info">
        <span>Sun 17 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info --><h1>Non-Parametric</h1>
<p>So far all models we have talked about have a fixed structure that is supported by parameters. What does that mean? Given a data set, we always suppose a model form in advance. For example, we have 0 order, 1 order,  for a given data.  </p>
<p><img alt="im30" src="/images/im30.png" />
<img alt="im31" src="/images/im31.png" />  </p>
<p>Also 2 order, 3 order polynomial for the same dataset.  </p>
<p><img alt="im32" src="/images/im32.png" />
<img alt="im33" src="/images/im33.png" />  </p>
<p>Now we want to allow flexibility in \( f(x)\) having local structure. It means we want fit locally to each data point. A simple example is 1-nearest-neighbor regression. This example outputs the predicted value = "closest" \( y_i\). Suppose we only have four data points, and the model we built based on 1-nearest-neighbor model looks like the following figure.  </p>
<p><img alt="im34" src="/images/im34.png" />  </p>
<p>For a query point: \( \hat{x}_q \), the model is to find the "closest" \( x_i\) in the dataset.  </p>
<div class="math">$$x_{min} \leftarrow min_i distance(x_i, x_q)$$</div>
<p>A natural question is what the distance is. For one dimensional data, a good candidate is absolute difference, or Euclidean distances. For high dimensional data we can have:  </p>
<div class="math">$$distance(x_i, x_q)=\sqrt{a_1(x_{i_1}-x_{{q_1}})^2 + a_2(x_{i_2}-x_{{q_2}})^2+...+a_d(x_{i_d}-x_{{q_d}})^2}$$</div>
<p>We will see that 1-NN model is sensitive to regions with little data and also sensitive to noise data. So a more reliable estimate is based on a larger set of comparable cases. We call it K-NN which is K-nearest neighbor for short. The idea is also intuitive and simple.  </p>
<p>For a given query point \( \hat{x}_q \):<br />
First, we find \( k\) closest \( x_i \) in dataset \( (x_i), i\in [1,k]\) such that for any \( x_i \) not in the nearest neighbor set we have:  </p>
<div class="math">$$distance(x_i, x_q)\ge distance(x_{NN_k}, x_q)$$</div>
<p>Second, we predict the value using:  </p>
<div class="math">$$\hat{y}_q=\frac{1}{k}(y_{NN_1}+y_{NN_2}+...y_{NN_k})=\frac{1}{k}\sum_{i=1}^{k}y_{NN_i}$$</div>
<p>Generally k-NN can fit the data pretty well. It may look like the following figure.  </p>
<p><img alt="im35" src="/images/im35.png" />  </p>
<p>But it also reveals some problesm. We will see discontinuities as window moves on. One point may fall out of the window completely. It will cause step form jump even for two close data. For example, there are two houses of 2450 and 2451 square feet respectively. The price may be very different because of the jump. Another issue is the boundary sparse region. Check the begining and end of the curve, we see the value is almost a constant deviating the data set severely. In a nutshell, the k-NN's overall predictive accuracy is OK but for some specific prediction may be really bad.  </p>
<h1>Weighted k-NN</h1>
<p>We have introduced k-NN model in previous section. Now we continue to fix the problem that k-NN has. Note that prediction value is generated under the assumption that each data point in the window contributes equally to the prediction. A better idea is assign larger weights to the similar or closer data points but smaller weights to the less similar or distant data points in the list of k-NN. The general output looks like:  </p>
<div class="math">$$y = \frac{w_1x_{NN_1}+w_2x_{NN_3}+...+w_kx_{NN_k}}{\sum weights}$$</div>
<p>The issue is how to define the each \(w \) such that: <br />
\( w\) is small when the \(distance(x_{NN_i}, x_q) \) is large and rather than vice versa. A intuitive idea is to use the inverse of the distance. As a matter of fact, it is time when kernel comes to help. We can define the weights as following:  </p>
<div class="math">$$kernel_{\lambda}(|x_{NN_j}|-x_q)$$</div>
<p>If we use the gaussian kernel, then it will be:  </p>
<div class="math">$$kernel_{\lambda}(|x_{NN_j}-x_q|)=exp^{\frac{-(x_i-x_q)^2}{\lambda}}$$</div>
<p>We have many other kernel stars which can be seen in the following figure:  </p>
<p><img alt="im36" src="/images/im36.png" /></p>
<p>Instead of just weighting nearest neighbor, we weigh all points and get our major hero today---kernel regression. Applause should be here! It looks like:  </p>
<div class="math">$$\hat{y}_q=\frac{\sum_{i=1}^{N}c_{q_i}y_i}{\sum_{i=1}^{N}c_{q_i}}=\frac{\sum_{i=1}^{N}kernel_{\lambda}(distance(x_i,x_q))y_i}{\sum_{i=1}^{N}kernel_{\lambda}(distance(x_i,x_q))}$$</div>
<p>It has another longer name in statistics: Nadaraya-Watson kernel weighted average. Kernel has bounded support so only subset of data is needed to compute local fit. The pracitical issue is how to choose \(\lambda \). So the following picture shows the influence of \( \lambda \). It is quite interesting! Thanks to Emily Fox, my teacher~  </p>
<p><img alt="im37" src="/images/im37.png" />  </p>
<p>Often the choice of kernel matters less than the choice of \( \lambda \). As we can see from the above figure, when \( \lambda \) decreases, we have a smaller region support; when \( \lambda \) increases, we have a larger region support. So smaller \( \lambda \) means large variance and larger \( \lambda \) means oversmoothed and has higher bias. Same story as befre, cross-validation help us to choose the proper \( \lambda \).  </p>
<p>In a more general view, kernel regression is more like a local linear regression. Local linear fit reduces bias at boundaries with minimum increase in variance. Local quadratic fit doesn't help at boundaries and increases variance but does help capture curvature in the interior. With sufficient data, local polynomials of odd degree dominate those of even degress.  </p>
<p>On the other hand, we have mentioned the non-parametric methods. k-NN and kernel regression are examples of non-parametric regression. Some characteristics of non-parametrics are: flexibility, few assumptions about \( f(\vec{x}) \) and complexity can grow largely with the number of observations N increases.  </p>
<h1>Summary For Regression</h1>
<p>Now curtains are falling down for the regression chapter. Even only using regression which have been talked so far, we can solve many problems now, no matter attending Kaggle or publishing papers. The below is a simple recap of what we have talked about.  </p>
<p>First we started from simple regression with only one attributes. Then multiple entries are introduced which requires multiple regression. And assessment of our models are based on the total lost function. To restrict our models not to be overfitting, we add \( l_1\)(LASSO) or \(l_2\) (ridge regression) penalty to the total lost. We covered 3 components of error to reveal the focus of almost all machine learning models: trade off between bias and variance. To achieve the balance and tune the control paramter, we introduced validation and cross-validation method. To solve our models we talked about two general approaches. The first is analytical solution or closed form solution. The second one is iteration method including gradient descent(with step)and coordinate descent(without step). At last we covered nearest negihbor and discussed non-parametric approaches with kernel regression included.  </p>
<p>Also there are so many other topics in regression which we may supplement later. For example, what if multiple outputs \( \vec{y}\) are desired in regression? What about aximum likelihood estimation for regression? When the errors follow normal distribution the result is similar to least squares methods. What if the errors follow other sorts of distribution? There is also another kind of regression model called regression trees. So stay hungry and stay foolish. Next chapter will be classification, in my view, is another super star problem in machine learning. Stay tuned~    </p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_5_lasso.html" rel="bookmark"
                           title="Permalink to Regression_5_LASSO">Regression_5_LASSO</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Fri 15 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>Previously we have talked about ridge regression in which we used norm 2 as the constraints to balance the bias and variance. In this article, we will use norm 1 which is called least absolute shrinkage and selection operator and LASSO for short.</p>
                <a class="readmore" href="/regression_5_lasso.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_4_ridge_regression.html" rel="bookmark"
                           title="Permalink to Regression_4_Ridge_Regression">Regression_4_Ridge_Regression</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Fri 08 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>Previously we have talked about simple regression and for a machine learning model we want to get a tradeoff between bias and variance. Now we introduce one tuning parameter and check how ridge regression works.</p>
                <a class="readmore" href="/regression_4_ridge_regression.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_3_simple_regression.html" rel="bookmark"
                           title="Permalink to Regression_3_Simple_Regression">Regression_3_Simple_Regression</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Tue 05 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>We have generic models and algorithms to solve them. But we always have to worry about how much i am losing. If our model prediction is too low, then we only get low offers; if model prediction is too high, we get few looks and no offers. So this article will presents how to assess our models.</p>
                <a class="readmore" href="/regression_3_simple_regression.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_2_multiple_regression.html" rel="bookmark"
                           title="Permalink to Regression_2_Multiple_Regression">Regression_2_Multiple_Regression</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sat 02 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>What is regression? In this article, i start from simple examples and extract out the basic model for regression problems. Then i will continue to talk about practical issues when we use the model to solve real problems.</p>
                <a class="readmore" href="/regression_2_multiple_regression.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_1_simple_regression.html" rel="bookmark"
                           title="Permalink to Regression_1_Simple_Regression">Regression_1_Simple_Regression</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Mon 21 March 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>What is regression? In this article, i start from simple examples and extract out the basic model for regression problems. Then i will continue to talk about practical issues when we use the model to solve real problems.</p>
                <a class="readmore" href="/regression_1_simple_regression.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/Introduction.html" rel="bookmark"
                           title="Permalink to Introduction">Introduction</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sun 20 March 2016</span>

</footer><!-- /.post-info -->                <p>About me</p>
                <a class="readmore" href="/Introduction.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/nothingnew.html" rel="bookmark"
                           title="Permalink to First-Blog">First-Blog</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sat 19 March 2016</span>
<span>| tags: <a href="/tag/pelican.html">pelican</a><a href="/tag/publishing.html">publishing</a></span>
</footer><!-- /.post-info -->                <p>Try some format</p>
                <a class="readmore" href="/nothingnew.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/re.html" rel="bookmark"
                           title="Permalink to First-Blog">First-Blog</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sat 19 March 2016</span>
<span>| tags: <a href="/tag/pelican.html">pelican</a><a href="/tag/publishing.html">publishing</a></span>
</footer><!-- /.post-info -->                <p>Try some format</p>
                <a class="readmore" href="/re.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
            </ol><!-- /#posts-list -->
<p class="paginator">
    Page 1 / 1
</p>
            </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://twitter.com/VicWeituo">twitter</a></li>
                            <li><a href="https://github.com/weituo12321">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <p>Powered by <a href="http://getpelican.com/">Pelican</a>. Theme <a href="https://github.com/blueicefield/pelican-blueidea/">blueidea</a>, inspired by the default theme.</p>
        </footer><!-- /#contentinfo -->

</body>
</html>