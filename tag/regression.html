<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>HardCoreAI - regression</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
<a href="https://github.com/weituo12321">
<img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub" />
</a>
        <header id="banner" class="body">
                <h1><a href="/">HardCoreAI </a></h1>
                <nav><ul>
                    <li><a href="/category/announcement.html">Announcement</a></li>
                    <li><a href="/category/machine_learning.html">Machine_Learning</a></li>
                    <li><a href="/category/trial.html">trial</a></li>
                </ul>
                </nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/regression_4_ridge_regression.html">Regression_4_Ridge_Regression</a></h1>
<footer class="post-info">
        <span>Fri 08 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info --><h1>Tuning Parameter</h1>
<p>We know that complext models have low bias but high variance, simple models have high bias but low variance. Also we know error consist of bias and variance plus noise. A trade off between bias and variance is what we need. Also we gave the formal definition of overfitting. Usually the symptom of overfitting is \( ||w||\) becomes very large. Think about it, there are a lot of elements in \( \vec{w}\) or each element is very large(it means the model is very sensitive to train data, any change in train data will cause a large change on model). On the other hand, how does number of observations influence overfitting? For few observations, as model complexity increases, the model overfits rapidly. For many obvervations, as model complexity increases, the model overfits slowly.  </p>
<p>So in addtion to the difference between real-value and predicted value, we add the magnitude of coefficients to our measure loss. So we now have:  </p>
<div class="math">$$Cost = fitness + magnitude$$</div>
<p>The first term is our old friend:RSS. The second term is so called regularizer which is used to control the model complexity. Speaking of magnitude of coefficients, which one should we use? norm 1 or norm 2? In ridge regression we use norm 2. Norm 1 is LASSO which we will talk later. So for ridge regression we have:  </p>
<div class="math">$$Cost = RSS(\vec{w})+\lambda||w||_{2}^2$$</div>
<p>"Call 911, our target tuning parameter lambda occurs" "Roger that~" Let's take a closer look at this lambda.  </p>
<ul>
<li>
<p>if \( \lambda=0\), the cost reduces to RSS as before. Suppose the solution is marked as \( \hat{w}^{LS}\) where super script \( LS\) means least square.  </p>
</li>
<li>
<p>if \(\lambda=\infty\), for solution where \(\hat{w}\ne 0\), the total cost is \(\infty \). So \(\hat{w}\) has to be 0, then the total cost is \(RSS(0)\).  </p>
</li>
<li>
<p>if \(\lambda\) is in between, then 0\(\le ||\hat{w}||_2^2 \le ||\hat{w}^{LS}||_2^2\)</p>
</li>
</ul>
<h1>Bias-Variance Tradeoff</h1>
<p>A natural quesiton is how to choose \( \lambda\). We take two steps to see what influence this tuning parameter \( \lambda\) has.   </p>
<p>First we check what happens to the feature coefficients \(\hat{w}^{LS}\). Thanks to my teacher we have a clear graph here to see. Note that this graph is built on the house_price of King County area of Seattle. The features of corresponding coefficients are square_living, number of bedrooms, etc. So different color of curve represents the coefficient of its feature. The horizontal axis means the \( \lambda\).  </p>
<p><img alt="im20" src="/images/im20.png" />  </p>
<p>As \(\lambda \) becomes larger, the magnitude of \( \hat{w}\) becomes smaller and smaller. If \( \lambda\) becomes infinity, the magnitude will shrink to 0.  </p>
<p>Second, we check what happens to the fit curve. Still the same data set, and we divide it into four sets. We compare the model fit curve with and without tunning parameter.  </p>
<p><img alt="overfitting1" src="/images/overfitting1.png" />
<img alt="ridge1" src="/images/ridge1.png" />  </p>
<p><img alt="overfitting2" src="/images/overfitting2.png" />
<img alt="ridge2" src="/images/ridge2.png" />  </p>
<p><img alt="overfitting3" src="/images/overfitting3.png" />
<img alt="ridge3" src="/images/ridge3.png" />  </p>
<p><img alt="overfitting4" src="/images/overfitting4.png" />
<img alt="ridge4" src="/images/ridge4.png" />  </p>
<p>We train a 15th order of polynomial model on the four sets of data. The images of left column show the model without tuning paramter. They are very good examples of overfitting. The images of right column show model fit with \( 10^5\). The curve becomes simple and well-behaved. Instead if we take a look at simple model like the following figure.  </p>
<p><img alt="im21" src="/images/im21.png" />  </p>
<p>The blue line is the original model and the red line is model with tuning parameter. It is obvious that the tuning parameter tries to tame the original curve, make it more conservative.  </p>
<p>Finally we check the influence tuning parameter has on the error.  </p>
<p><img alt="im22" src="/images/im22.png" />  </p>
<p>It looks like an "U". So the bottom of the "U" is our sweet point. In practical, we first fix proper range of \(\lambda \) by tuning it in logarithmic step, then search the sweet point in the proper range.  </p>
<h1>Get Maximum/Minimum</h1>
<p>Since we have added a tuning parameter into our cost function. How to get the maximum/minimum? Still, the gradient descent works here. Remember gradient descent and its variants are powerful methods in convex optimization problems.  </p>
<p>We start from the cost function in matrix form.  </p>
<div class="math">$$RSS(\vec{w})=(\vec{y}-Hw)^{T}(y-Hw)$$</div>
<div class="math">$$||w||_{2}^2=\vec{w}^{T}\vec{w}$$</div>
<p>So the total cost will be:  </p>
<div class="math">$$(\vec{y}-Hw)^{T}(y-Hw)+\lambda\vec{w}^{T}\vec{w}$$</div>
<p>We take the gradient and get:  </p>
<div class="math">$$-2H^{T}(\vec{y}-\vec{H}w)+2\lambda w$$</div>
<p>Set the gradient = 0:  </p>
<div class="math">$$\triangledown cost(w) = -2H^{T}(\vec{y}-\vec{H}w)+2\lambda I w = 0$$</div>
<p>Solve it and get:  </p>
<div class="math">$$\hat{w}^{ridge}=(H^{T}\vec{H}+\lambda I)^{-1}H^{T}\vec{y}$$</div>
<p>If \( \lambda=0\),then \( \hat{w}^{ridge}=(H^{T}H)^{-1}H^{T}y=\hat{w}^{LS}\)  </p>
<p>if \( \lambda=\infty\), then \( \hat{w}^{ridge}=0\). It's like divded by \(\infty\)  </p>
<p>Note that the term \(\lambda I\) in \( \hat{w}^{ridge}\) will always make \((H^{T}\vec{H}+\lambda I) \) invertible if \( \lambda &gt; 0\) even if \( N&lt;D\) where \(N\) is the numebr of data points and \( D\) is how many features we consider. Unfortunately the complexity of inverse is \( O(D^3)\). So we turn to gradient descent.  </p>
<p>Take a look at the jth feature weight:  </p>
<div class="math">$$w_{j}^{(t+1)}\leftarrow w_{j}^{(t)}-\eta[-2\sum_{i=1}^{N}h_{j}(x_{i})(y_i-\hat{y_i}(w^{t}))+2\lambda w_{j}^{(t)}]$$</div>
<p>That is:  </p>
<div class="math">$$w_{j}^{(t+1)}\leftarrow (1-2\eta \lambda)w_{j}^{(t)}+2\eta h_{j}(x_{i})(y_i-\hat{y_i}(w^{t}))$$</div>
<p>Note that the second term of the right side is the same as the term we use to update the original RSS. The difference is the \( -2\eta \lambda \). It tells that on the basis of the original update, we minus a term, so the effect is like shrinking the result. The process help us avoid that w becomse too large. So the track will be like vibration before we reach the destination.  </p>
<h1>Practical Issues</h1>
<h4>Cross-validation</h4>
<p>We have said validation is used to choose \( \lambda\). If our data amount is not enough, cross-validation comes to our mind. We divide randomly train data into \( K\) folders, pick out \( K-1\) set to train our model and use the last group to validate. Iterate this process on every folder of data. So at last we will have \( K \) errors. And for this specific \(\lambda \) we can have :  </p>
<div class="math">$$CV(\lambda)=\frac{1}{K}\sum_{k=1}^{K}error_{k}(\lambda)$$</div>
<p>So the task is to find \(\lambda^*\) to minimize the \( CV(K)\). How many folders should we use? If \(K=N \), it is leave-one-out cross validtion. Typically, let K=5 or 10. </p>
<h4>Handle the intercept</h4>
<p>Given a vector of coefficients, \( \lambda\) penalize the constant feature, making \( w_0\) small. It is not necessary. So we have two options. First one is leave the intercept alone. The modified gradient descent is:  </p>
<hr />
<p>while \( ||\triangledown RSS(w^{(t)})||&gt;\epsilon\):<br />
&ensp;&ensp;&ensp;&ensp;for j=0,...D:<br />
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;partial[j] = \(-2\sum_{i=1}^{N}h_j(x_i)(y_i-\hat{y_i}(w^{(t)})) \)  </p>
<p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;if j = 0:<br />
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;\( w_0^{(t+1)}\leftarrow w_0^{t}-\eta *partial[j]\)<br />
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;else:  </p>
<p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp; \( w_0^{(t+1)}\leftarrow (1-2\eta \lambda)w_0^{(t)}- \eta *partial[j] \)<br />
\( t\leftarrow t+1\)  </p>
<hr />
<p>Another option is center the data first. It means transform y to have 0 mean.Then run nomal ridge regression on the normalized y.  </p>
<h1>Summary</h1>
<p>In this article we talked start from the tradeoff between bias and variance. To achieve the balance we introduce tuning parameter \( \lambda\) to control the model complexity. It shrinks the feature coefficients when gradient descent is carried out. Note that the cost function now is the sum of RSS and magnitude of the coefficients. For ridge regression the magnitude is calculated by norm 2. Next we will talk about using norm 1. It will be the "least absolute shrinkage and selection operator", LASSO for short.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_3_simple_regression.html" rel="bookmark"
                           title="Permalink to Regression_3_Simple_Regression">Regression_3_Simple_Regression</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Tue 05 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>We have generic models and algorithms to solve them. But we always have to worry about how much i am losing. If our model prediction is too low, then we only get low offers; if model prediction is too high, we get few looks and no offers. So this article will presents how to assess our models.</p>
                <a class="readmore" href="/regression_3_simple_regression.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_2_multiple_regression.html" rel="bookmark"
                           title="Permalink to Regression_2_Multiple_Regression">Regression_2_Multiple_Regression</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sat 02 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>What is regression? In this article, i start from simple examples and extract out the basic model for regression problems. Then i will continue to talk about practical issues when we use the model to solve real problems.</p>
                <a class="readmore" href="/regression_2_multiple_regression.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_1_simple_regression.html" rel="bookmark"
                           title="Permalink to Regression_1_Simple_Regression">Regression_1_Simple_Regression</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Mon 21 March 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>What is regression? In this article, i start from simple examples and extract out the basic model for regression problems. Then i will continue to talk about practical issues when we use the model to solve real problems.</p>
                <a class="readmore" href="/regression_1_simple_regression.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
            </ol><!-- /#posts-list -->
<p class="paginator">
    Page 1 / 1
</p>
            </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://twitter.com/VicWeituo">twitter</a></li>
                            <li><a href="https://github.com/weituo12321">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <p>Powered by <a href="http://getpelican.com/">Pelican</a>. Theme <a href="https://github.com/blueicefield/pelican-blueidea/">blueidea</a>, inspired by the default theme.</p>
        </footer><!-- /#contentinfo -->

</body>
</html>