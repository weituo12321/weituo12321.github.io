<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>HardCoreAI - classification</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
<a href="https://github.com/weituo12321">
<img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub" />
</a>
        <header id="banner" class="body">
                <h1><a href="/">HardCoreAI </a></h1>
                <nav><ul>
                    <li><a href="/category/announcement.html">Announcement</a></li>
                    <li><a href="/category/machine_learning.html">Machine_Learning</a></li>
                    <li><a href="/category/trial.html">trial</a></li>
                </ul>
                </nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/classification_5_large_datasets_and_evaluation.html">Classification_5_Large_Datasets_and_Evaluation</a></h1>
<footer class="post-info">
        <span>Sat 30 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info --><h1>Stochastic Gradient Descent</h1>
<p>Gradient descent is slow! It is because every iteration we need to go through the whole dataset. Since we are ushering into so called big data era, simple models can performs better than fancy models. However, as distribuetd computing and GPU advances, now we can also run complex models. That's why we see boosted trees, tensor factorization, deep learning, massive graphical models running these days. Scaling to huge datasets, we switch from gradient descent to stochastic gradient descent(SGD). There is a story about SGD. SGD represents <em>Student Gradient Descent</em>. It means assistant professors' goal is to get tneure. He or she has to choose students to work with and publish good papers. Year by year, new students come, and assistant professors must choose student, and publish papers. Finally, they get the tenure~ It is very similar to what true SGD does because we choose some data points to update our model. Take logistic regression for example, we have:  </p>
<p><img alt="im" src="/images/clim14.png" />  </p>
<p>Instead of using the sum of all the data points, we just use only one data point to update the coefficients. A basic comparison between gradient descent and stochastic gradient descent can be seen as follows:  </p>
<p><img alt="im" src="/images/clim15.png" />  </p>
<p>But why SGD can work? Usually gradient is "best" direction but any direction helps convergence would be helpful. Most data points can increase the gradient. On average, it makes progress.  </p>
<p>Some practical issues on SGD:<br />
First, the order of data can introduce bias. So usual way is to shuffle data before running SGD.<br />
Second, step size \( \eta\) matters. If it is too small, SGD is very slow to converge. If it is too large, SGD oscillates a lot and much worse than GD. We have talked about how to choose the proper step size in previous discussions. (Get lowerbound and upperbound in exponetially space and make step size decrease as iteration increases).<br />
Third, DO NOT trust the last coefficients by SGD. Instead, use the average weights. For example, \(\hat{w}=\frac{1}{T} \sum_{i=1}^{T}\hat{w}_i \).  </p>
<h1>Learning From Batches</h1>
<p>In previous SGD, we update gradient by one data points. Here we update gradient by a batch of data. For example, batch size = 100. In this way, we can decrease noise and make our model more stable. Still, large batch size causes slow convergence and small batch size causes too much noise and even no convergence. So SGD with mini-batches is like follows:  </p>
<hr />
<p>Shuffle data<br />
Init \(w^{(1)} = 0, t = 1 \)<br />
Until converged:<br />
&ensp;&ensp;&ensp;for k=0,...N/B - 1:(for each mini-batch)<br />
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;for j=0,...D:<br />
</p>
<div class="math">$$w_j^{(t+1)} \leftarrow w_j^{(t)}+\eta \sum_{i=1+k*B}^{(k+1)B}\frac{\partial l_i(w)}{\partial w_j}$$</div>
<p><br />
&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;\(t\leftarrow t+1 \)  </p>
<hr />
<p>A pracitical issue is how to measure the convergence. We can compute log-likelihood of single data point, then for a iteration t we use the average log-likelihood of previous say 70 data points. It's like a sliding window.  </p>
<h1>Regularization</h1>
<p>In common logistic regression we can compute the total derivative as \( \sum_{i=1}^{N} \frac{\partial l_i(w)}{\partial w_j} - 2 \lambda w_j \). In SGD, we can compute the total derivative as:<br />
</p>
<div class="math">$$\sum_{i=1}^{N} \frac{\partial l_i(w)}{\partial w_j} - \frac{2}{N} \lambda w_j $$</div>
<h1>Online Learning</h1>
<p>SGD is very useful in online learning. Online learning is geard  towards the situation where models must be trained as new data comes. For example, advertisement targeting requires updating coefficients immediately. The good thing is that model is always up to date and often more accurate and don't need to store all data. The bad thing is the overall system is complex and cost a lot to money. So most companies opt for systems that save data and update coefficients every night or every week.  </p>
<h1>Precision and Recall</h1>
<p>We used error and accuracy to evaluate the regression and classification model. Sometimes these criteria is not perfect for classifiers. One may ask: what does it mean for good performance? For example, if we own a restaurant and our website shows 10 sentences from recent reviews. Precision means "Did i mistakenly show a negative sentence?" and Recall means "Did i not show a great positive sentence?" Accuracy doesn't capture these issues well.  </p>
<h4>Formal definition</h4>
<p>precision: fraction of positive predictions are really predictions.
For example, we predicted \(y_1=+1,y_2=-1,y_3=+1\) three data points as +1. So the precision is \( \frac{2}{3}\). It means precision = true positive / (true positive + false positive). High precision means positive predictions are likely to be true positive   </p>
<p>recall: fraction of positive data predicted to be postive. It means recall = true positive / (true positive + false negative). High recall means positive data are likely to be discovered.  </p>
<h4>precision_recall evaluation</h4>
<p>Given a model that has high recall and low precision, we say it is optimistic model. For example, we predict all the data to be  positive, then the recall will be 1 but the precision will be very low.  </p>
<p>Given a model that has high precision but low recall, we say it is a pessimistic model.For example, we often predict data to be negative and only a few data to be positve. So the precision may be 1 but the recall is low.  </p>
<p>We need to adjust our model between optimistic and pessimistic model. An extra parameter \(\lambda \) can be used. For example, if \(\hat{P}(y=+1|x_i)&gt; \lambda \), \(\hat{y}_i = +1,else \hat{y}_i=-1 \). We will see that if \(\lambda=0.99 \) the model is pessimistic. If \( \lambda=0.01\), the model is positive.  </p>
<p>We can draw precision versus recall in a 2D plane and adjust the \(\lambda \) to see the performance of a model as follows:  </p>
<p><img alt="im" src="/images/clim16.png" />  </p>
<p>Seen from the figure above, model B is better. What if two curves ross each other? We can use some other measures like F1 measure or area_under_the_curve(AUC). We often fix the number of predictions to show. For example, only 5 sentences are showing on the restaurant website, and then the precision is at k=5.  </p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="/classification_4_boosting.html" rel="bookmark"
                           title="Permalink to Classification_4_Boosting">Classification_4_Boosting</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Thu 28 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info -->                <p>We will talk about the combination of weak classifiers can make big difference in classification. We call it boosting.</p>
                <a class="readmore" href="/classification_4_boosting.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/classification_3_decision_trees.html" rel="bookmark"
                           title="Permalink to Classification_3_Decision_Trees">Classification_3_Decision_Trees</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Wed 27 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info -->                <p>Previously we always played with models having a linear combination structure. This article will introduce a totally different model. Decision trees are so simple and so powerful that ada-boosting based on decision trees is often the key to win competitions.</p>
                <a class="readmore" href="/classification_3_decision_trees.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/classification_1_linear_classifiers.html" rel="bookmark"
                           title="Permalink to Classification_1_Linear_Classifiers">Classification_1_Linear_Classifiers</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sat 23 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info -->                <p>What is classification? In previous chapter we have talked about regression. Instead of real values, classification outputs predictions as some boolean values or labels. Huge amount of problems in machine learning are classification problems such as sentiment analysis, spam filtering, webpage analysis, image classification, personalized medical diagnosis, etc. The models we will cover in this chapter will be linear classifiers, logistic regression, decision trees, and ensemble methods. If time is allowed, support vector machines will also be included. The algorithms to solve the models are gradient descent, stochastic gradient descent, recursive greedy and boosting. Some practical issues like handling missing data, precision-recall tradeoff, and online learning will also be talked about.</p>
                <a class="readmore" href="/classification_1_linear_classifiers.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/classification_2_linear_classifiers_2.html" rel="bookmark"
                           title="Permalink to Classification_2_Linear_Classifiers_2">Classification_2_Linear_Classifiers_2</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sat 23 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info -->                <p>In previous article, we have talked about the basic linear classifiers and some practical issues. So we start from the metric of the models in terms of maximum likelihood estimation.</p>
                <a class="readmore" href="/classification_2_linear_classifiers_2.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
            </ol><!-- /#posts-list -->
<p class="paginator">
    Page 1 / 1
</p>
            </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://twitter.com/VicWeituo">twitter</a></li>
                            <li><a href="https://github.com/weituo12321">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <p>Powered by <a href="http://getpelican.com/">Pelican</a>. Theme <a href="https://github.com/blueicefield/pelican-blueidea/">blueidea</a>, inspired by the default theme.</p>
        </footer><!-- /#contentinfo -->

</body>
</html>