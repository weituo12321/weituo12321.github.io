<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>HardCoreAI - classification</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
<a href="https://github.com/weituo12321">
<img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub" />
</a>
        <header id="banner" class="body">
                <h1><a href="/">HardCoreAI </a></h1>
                <nav><ul>
                    <li><a href="/category/announcement.html">Announcement</a></li>
                    <li><a href="/category/machine_learning.html">Machine_Learning</a></li>
                    <li><a href="/category/trial.html">trial</a></li>
                </ul>
                </nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/classification_3_decision_trees.html">Classification_3_Decision_Trees</a></h1>
<footer class="post-info">
        <span>Wed 27 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info --><h1>Decision Tree</h1>
<p>Think about a problem: what makes a load risky? We may list factors like credit, income, term, personla info and so on. We call these factors features or attributes. So the basic schema of decision tree as follows:  </p>
<hr />
<p>step 1: start with an empty tree<br />
step 2: select a feature to split data<br />
for each split of the tree:<br />
&ensp;&ensp;&ensp;step 3: if nothing more to do, make predictions<br />
&ensp;&ensp;&ensp;step 4: otherwise, go to step 2 and continue to do split(recursively)  </p>
<hr />
<p>From the basic steps, we have two problems: 1 how to choose the feature? 2 when to stop the recursive process? For the first question we can use the classification error to sift out the best feature.  </p>
<p>To make a prediction we follow the majority vote principal. The major class will represent the class of the split. For example we have credit and term to consider the split. We can calculate classification error respectively and choose the better one. </p>
<p><img alt="clim11" src="/images/clim11.png" />  </p>
<p>We don't mention the entropy here since the effect will be the same. Entropy stuff will be provided later.  </p>
<p>Another practical issue is how to deal with the continous numeric data. The answer is very natural: just use a threshold to split. The way to find the threshold is also simple.  </p>
<hr />
<p>step 1: sort the values of a feature \(h_j(x) \)<br />
&ensp;&ensp;&ensp;let \(v_1,v_2,...v_N \) denote sorted values.  </p>
<p>step 2: For \(i=1,...N-1 \)<br />
&ensp;&ensp;&ensp; consider split \(t_i=(v_i+v_{i+1})/2 \)<br />
&ensp;&ensp;&ensp; compute classification error for threshold split \(h_j(x) &gt;= t_i \)  </p>
<p>&ensp;&ensp;&ensp; choose the \( t^*\) with the lowest classification error  </p>
<hr />
<p>Note that for the threshold split, we can reuse the features!  For the predictions we can also consider them as probabilities of the corresponding result. A key note for students who are struggling for machine learning tests: XOR is always a good example as an opposite. So when you come across questions like "list a opposite example blablabla" think about the XOR table.  </p>
<h1>Preventing Overfitting</h1>
<p>Training error reduces as depth increases but it's highly possible to be overfitting.  </p>
<p><img alt="clim12" src="/images/clim12.png" /></p>
<p>It is because every split will choose the lowest classification error which means reduce the training error. 
According to <em>Occam Razor</em>, simpler trees means better model. For those who are not familar with <em>Occam Razor</em>, we give a brief note here: Among competing hypotheses, the one with fewest assumptions should be selected. We will cover it in more details in statistic folder. For example, we have model_1 and model_2. Model_1 has training error of 0.12 and validation error of 0.15. Model_2 has training error of 0.07 and validation error of 0.15. We will pick the model_1 since both of the two have the same validation error but the model_1 is a simpler one. To pick a simpler tree, we have two ways: The first one is stop learning algorithm before tree become too complex, which is early stopping. The second one is simplify tree after learning algorithm terminates, which is pruning.  </p>
<h4>Early stopping</h4>
<p>Condition 1: We can limit the depth of a tree. To set the maximum depth of a tree we can use the validation method. Condition 2: We can use classification error to limit the tree depth. If the classification error will not reduce as split goes on, we should stop. So a threshold error needs to be set in advance. Condition 3: We can stop if number of data points in a node is too small. So a minimum data point number should be set in advance.  </p>
<h4>Pruning</h4>
<p>The normal stopping conditions: all examples have the same target value and no more features to split on. We have introduced some early stopping conditions but there are some challenges with early stopping conditions. For example, for the early stopping condition 1, it is actually hard to know exactly when and where to stop. For the early stopping condition 2, we may miss good split after the useless split. To prove it, we can use XOR example here.(like i said, XOR is really a good opposite example for many problems). So the idea becomes: first train a complete tree, then prune it!  </p>
<p>Formalization can be written as follows:  </p>
<div class="math">$$cost = error(T) + \lambda L(T)$$</div>
<p>where \(error(T) \) means the classification error and \(L(T) \) is the number of leaves. If \( \lambda = 0\), then it becomes a standard decision tree learning. If \(\lambda = \infty \), then \( \hat{y} = majority\). If \(\lambda \) in between, we get a balance between fit and complexity.  </p>
<h1>Handling Missing Data</h1>
<p>Strategy 1: Purification by skipping<br />
If a few points contains unknown values, skip those points.<br />
If a lot of points contain unknown values in certain feature, then skip that feature.<br />
This idea is easy to understand but may lose important information.  </p>
<p>Strategy 2: Purification by inputting missing data<br />
Input rules 1<br />
For categorical features, we use the majority value.<br />
Input rules 2<br />
For numerical features, we use the average or median.<br />
There are some advanced methods like EM.<br />
This idea is easy to understand and can be applied to many models, but may cause systematic errors. For example, feature "age" is missing in all banks in Washingto by state law.  </p>
<p>Strategy 3: Adapt learning algorithm to be robust to missing data<br />
For example, in decision trees, we associate "unknown" to one branch. So every decision tree node includes choice of response to missing data. This idea is good for prediction but we have to modify the whole model. Another practical issue is where to put the missing data?(i.e. associate which branch.) The solution is still choose branches which lead to least classification error.  </p>
<p>Single decision tree or simple models seem very weak. Next we will talk about how to combine them to make a strong classifier.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="/classification_1_linear_classifiers.html" rel="bookmark"
                           title="Permalink to Classification_1_Linear_Classifiers">Classification_1_Linear_Classifiers</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sat 23 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info -->                <p>What is classification? In previous chapter we have talked about regression. Instead of real values, classification outputs predictions as some boolean values or labels. Huge amount of problems in machine learning are classification problems such as sentiment analysis, spam filtering, webpage analysis, image classification, personalized medical diagnosis, etc. The models we will cover in this chapter will be linear classifiers, logistic regression, decision trees, and ensemble methods. If time is allowed, support vector machines will also be included. The algorithms to solve the models are gradient descent, stochastic gradient descent, recursive greedy and boosting. Some practical issues like handling missing data, precision-recall tradeoff, and online learning will also be talked about.</p>
                <a class="readmore" href="/classification_1_linear_classifiers.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/classification_2_linear_classifiers_2.html" rel="bookmark"
                           title="Permalink to Classification_2_Linear_Classifiers_2">Classification_2_Linear_Classifiers_2</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sat 23 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info -->                <p>In previous article, we have talked about the basic linear classifiers and some practical issues. So we start from the metric of the models in terms of maximum likelihood estimation.</p>
                <a class="readmore" href="/classification_2_linear_classifiers_2.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
            </ol><!-- /#posts-list -->
<p class="paginator">
    Page 1 / 1
</p>
            </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://twitter.com/VicWeituo">twitter</a></li>
                            <li><a href="https://github.com/weituo12321">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <p>Powered by <a href="http://getpelican.com/">Pelican</a>. Theme <a href="https://github.com/blueicefield/pelican-blueidea/">blueidea</a>, inspired by the default theme.</p>
        </footer><!-- /#contentinfo -->

</body>
</html>