<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>Classification_4_Decision_Boosting</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
<a href="https://github.com/weituo12321">
<img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub" />
</a>
        <header id="banner" class="body">
                <h1><a href="/">HardCoreAI </a></h1>
                <nav><ul>
                    <li><a href="/category/announcement.html">Announcement</a></li>
                    <li class="active"><a href="/category/machine_learning.html">Machine_Learning</a></li>
                    <li><a href="/category/trial.html">trial</a></li>
                </ul>
                </nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="/classification_4_decision_boosting.html" rel="bookmark"
           title="Permalink to Classification_4_Decision_Boosting">Classification_4_Decision_Boosting</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <span>Thu 28 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info -->      <h1>Boosting</h1>
<p>As we konw from the discussion on the balance betweeen bias and variance, simple or weak classifiers have low variance but high bias. So an old question is: can a set of weak learners be combined to create a stronger learner? Then answer is YES! Ensemble method is the idea. It is a simple but very powerful approach and widely used in industry and win most Kaggle competitions. 
Ensemble methods can be displayed as follows:  </p>
<p><img alt="clim13" src="/images/clim13.png" />  </p>
<p>General idea of ensemble methods is to train classifiers \(f_1(x), f_2(x),...f_T(x) \) and their coefficients \(\hat{w}_1, ... \hat{w}_T \) and then make the predictions:  </p>
<div class="math">$$\hat{y}=sign(\sum_{t=1}^T\hat{w}_tf_t(\textbf{x}))$$</div>
<p>Boosting is based on the ensemble idea. The simple classifier is decision stump which is decision tree of depth one. And boosting focus learning on points lead to high error. The idea is we first assign each data point weights and after the training, we will change the weights of each data points. We will increase the weight of wrong prediction. It's a general way in machine learning and we can take logistic regression as an example:  </p>
<div class="math">$$w_j^{(t+1)} \leftarrow w_j^{t}+\eta \sum_{i=1}^{N} \alpha_ih_j(x_i)(I[y_i = +1] - P(y=+1|x_i,w))$$</div>
<p>The \( \alpha_i\) is the weight of data point i.  </p>
<p>Naturally, there are two practical problems. First, how to compute the coefficient \( \hat{w}_t\). Second, how to recompute weights \( \alpha_i \)  </p>
<p>For the problem 1:<br />
We introduce weighted classification error. The total weight of mistakes is <br />
</p>
<div class="math">$$ \sum_{i=1}^{N}\alpha_iI(\hat{y}_i \ne y_i) $$</div>
<p>Total weight of all points is \( \sum_{i=1}^{N}\alpha_i\), and then we have weighted error to measure fraction of weight of mistakes: <em>weighted_error</em> = <em>total weight of mistakes</em> / <em>total weight of all data points</em>  </p>
<p>The best model's weighted error wil be 0 and the worst will be 1. And the random classifier's weighted error is 0.5.  </p>
<p>After we have the weighted error, we can compute the coefficients \( \hat{w}_t\) of classifier f_t(x) by following formula:  </p>
<div class="math">$$\hat{w}_t=\frac{1}{2}ln(\frac{1-error_{weighted}(f_t)}{error_{weighted}(f_t)})$$</div>
<p>Note that if a classifier's weighted error is too high close to 1 and maintains that "bad" performance, it is actually a awesome classifier since the 1 - <em>bad performance</em> is a pretty good result. This is so called jinx effect.  </p>
<p>For problem 2:<br />
if we already have coefficients \( \hat{w}_t\) we can update \(\alpha_i \) where classifier makes mistakes as follows:  </p>
<p>if \( f_t(x_i)=y_i \), we have \( \alpha_i \leftarrow \alpha_ie^{-\hat{w}_t} \)  </p>
<p>if \( f_t(x_i) \ne y_i \), we have \( \alpha_i \leftarrow \alpha_ie^{\hat{w}_t} \)  </p>
<p>However, if data point \( x_i\) is often mistakenly predicted, then its weights \( \alpha_i \) will be very large. On the contrast, it will be very small. This causes numerical instability after many iterations. So a better way is to normalize weights to add up to 1 after every iteration as follows:<br />
</p>
<div class="math">$$\alpha_i = \frac{\alpha_i}{\sum_{i=1}^{N}\alpha_i}$$</div>
<p>Thus we have a general procedure for boosted decison stumps:  </p>
<hr />
<p>Start same weights \(\alpha_i =\frac{1}{N} \)<br />
For t = 1,...T:<br />
&ensp;&ensp;&ensp;pick up the feature that results in least weighted_error<br />
&ensp;&ensp;&ensp;compute the coefficient \(\hat{w} \) for this classifier<br />
&ensp;&ensp;&ensp;then recompute weights \( \alpha_i\)<br />
&ensp;&ensp;&ensp;normalize \(\alpha_i \)  </p>
<hr />
<p>We will see as iteration goes on, the training error of boosted classifier will get close to 0. The condition is every iteration we can find a weak learner with weighted_error smaller than 0.5. Compare the gap between train error and test error of boosted decision tumps with that of decision trees, we will see the former one is smaller. As for the iteration times, we can use validation method similar to choose \( \lambda\) in previous contents.  </p>
<p>Except for Ada boosting, there are other boosting like gradient boosting, also other ensemble methods like random forests(bagging: take a sub set of the data, learn a tree in each subset and average the predictions). Next we will talk about practical issues for huge datasets and better measurements for classifiers. Stay tuned~</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://twitter.com/VicWeituo">twitter</a></li>
                            <li><a href="https://github.com/weituo12321">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <p>Powered by <a href="http://getpelican.com/">Pelican</a>. Theme <a href="https://github.com/blueicefield/pelican-blueidea/">blueidea</a>, inspired by the default theme.</p>
        </footer><!-- /#contentinfo -->

</body>
</html>