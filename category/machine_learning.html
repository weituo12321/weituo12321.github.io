<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>HardCoreAI - Machine_Learning</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
<a href="https://github.com/weituo12321">
<img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub" />
</a>
        <header id="banner" class="body">
                <h1><a href="/">HardCoreAI </a></h1>
                <nav><ul>
                    <li><a href="/category/announcement.html">Announcement</a></li>
                    <li class="active"><a href="/category/machine_learning.html">Machine_Learning</a></li>
                    <li><a href="/category/trial.html">trial</a></li>
                </ul>
                </nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/classification_4_boosting.html">Classification_4_Boosting</a></h1>
<footer class="post-info">
        <span>Thu 28 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info --><h1>Boosting</h1>
<p>As we konw from the discussion on the balance betweeen bias and variance, simple or weak classifiers have low variance but high bias. So an old question is: can a set of weak learners be combined to create a stronger learner? Then answer is YES! Ensemble method is the idea. It is a simple but very powerful approach and widely used in industry and win most Kaggle competitions. 
Ensemble methods can be displayed as follows:  </p>
<p><img alt="clim13" src="/images/clim13.png" />  </p>
<p>General idea of ensemble methods is to train classifiers \(f_1(x), f_2(x),...f_T(x) \) and their coefficients \(\hat{w}_1, ... \hat{w}_T \) and then make the predictions:  </p>
<div class="math">$$\hat{y}=sign(\sum_{t=1}^T\hat{w}_tf_t(\textbf{x}))$$</div>
<p>Boosting is based on the ensemble idea. The simple classifier is decision stump which is decision tree of depth one. And boosting focus learning on points lead to high error. The idea is we first assign each data point weights and after the training, we will change the weights of each data points. We will increase the weight of wrong prediction. It's a general way in machine learning and we can take logistic regression as an example:  </p>
<div class="math">$$w_j^{(t+1)} \leftarrow w_j^{t}+\eta \sum_{i=1}^{N} \alpha_ih_j(x_i)(I[y_i = +1] - P(y=+1|x_i,w))$$</div>
<p>The \( \alpha_i\) is the weight of data point i.  </p>
<p>Naturally, there are two practical problems. First, how to compute the coefficient \( \hat{w}_t\). Second, how to recompute weights \( \alpha_i \)  </p>
<p>For the problem 1:<br />
We introduce weighted classification error. The total weight of mistakes is <br />
</p>
<div class="math">$$ \sum_{i=1}^{N}\alpha_iI(\hat{y}_i \ne y_i) $$</div>
<p>Total weight of all points is \( \sum_{i=1}^{N}\alpha_i\), and then we have weighted error to measure fraction of weight of mistakes: <em>weighted_error</em> = <em>total weight of mistakes</em> / <em>total weight of all data points</em>  </p>
<p>The best model's weighted error wil be 0 and the worst will be 1. And the random classifier's weighted error is 0.5.  </p>
<p>After we have the weighted error, we can compute the coefficients \( \hat{w}_t\) of classifier f_t(x) by following formula:  </p>
<div class="math">$$\hat{w}_t=\frac{1}{2}ln(\frac{1-error_{weighted}(f_t)}{error_{weighted}(f_t)})$$</div>
<p>Note that if a classifier's weighted error is too high close to 1 and maintains that "bad" performance, it is actually a awesome classifier since the 1 - <em>bad performance</em> is a pretty good result. This is so called jinx effect.  </p>
<p>For problem 2:<br />
if we already have coefficients \( \hat{w}_t\) we can update \(\alpha_i \) where classifier makes mistakes as follows:  </p>
<p>if \( f_t(x_i)=y_i \), we have \( \alpha_i \leftarrow \alpha_ie^{-\hat{w}_t} \)  </p>
<p>if \( f_t(x_i) \ne y_i \), we have \( \alpha_i \leftarrow \alpha_ie^{\hat{w}_t} \)  </p>
<p>However, if data point \( x_i\) is often mistakenly predicted, then its weights \( \alpha_i \) will be very large. On the contrast, it will be very small. This causes numerical instability after many iterations. So a better way is to normalize weights to add up to 1 after every iteration as follows:<br />
</p>
<div class="math">$$\alpha_i = \frac{\alpha_i}{\sum_{i=1}^{N}\alpha_i}$$</div>
<p>Thus we have a general procedure for boosted decison stumps:  </p>
<hr />
<p>Start same weights \(\alpha_i =\frac{1}{N} \)<br />
For t = 1,...T:<br />
&ensp;&ensp;&ensp;pick up the feature that results in least weighted_error<br />
&ensp;&ensp;&ensp;compute the coefficient \(\hat{w} \) for this classifier<br />
&ensp;&ensp;&ensp;then recompute weights \( \alpha_i\)<br />
&ensp;&ensp;&ensp;normalize \(\alpha_i \)  </p>
<hr />
<p>We will see as iteration goes on, the training error of boosted classifier will get close to 0. The condition is every iteration we can find a weak learner with weighted_error smaller than 0.5. Compare the gap between train error and test error of boosted decision tumps with that of decision trees, we will see the former one is smaller. As for the iteration times, we can use validation method similar to choose \( \lambda\) in previous contents.  </p>
<p>Except for Ada boosting, there are other boosting like gradient boosting, also other ensemble methods like random forests(bagging: take a sub set of the data, learn a tree in each subset and average the predictions). Next we will talk about practical issues for huge datasets and better measurements for classifiers. Stay tuned~</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="/classification_3_decision_trees.html" rel="bookmark"
                           title="Permalink to Classification_3_Decision_Trees">Classification_3_Decision_Trees</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Wed 27 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info -->                <p>Previously we always played with models having a linear combination structure. This article will introduce a totally different model. Decision trees are so simple and so powerful that ada-boosting based on decision trees is often the key to win competitions.</p>
                <a class="readmore" href="/classification_3_decision_trees.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/classification_1_linear_classifiers.html" rel="bookmark"
                           title="Permalink to Classification_1_Linear_Classifiers">Classification_1_Linear_Classifiers</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sat 23 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info -->                <p>What is classification? In previous chapter we have talked about regression. Instead of real values, classification outputs predictions as some boolean values or labels. Huge amount of problems in machine learning are classification problems such as sentiment analysis, spam filtering, webpage analysis, image classification, personalized medical diagnosis, etc. The models we will cover in this chapter will be linear classifiers, logistic regression, decision trees, and ensemble methods. If time is allowed, support vector machines will also be included. The algorithms to solve the models are gradient descent, stochastic gradient descent, recursive greedy and boosting. Some practical issues like handling missing data, precision-recall tradeoff, and online learning will also be talked about.</p>
                <a class="readmore" href="/classification_1_linear_classifiers.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/classification_2_linear_classifiers_2.html" rel="bookmark"
                           title="Permalink to Classification_2_Linear_Classifiers_2">Classification_2_Linear_Classifiers_2</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sat 23 April 2016</span>
<span>| tags: <a href="/tag/classification.html">classification</a></span>
</footer><!-- /.post-info -->                <p>In previous article, we have talked about the basic linear classifiers and some practical issues. So we start from the metric of the models in terms of maximum likelihood estimation.</p>
                <a class="readmore" href="/classification_2_linear_classifiers_2.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_6_nearest-neighbor-and-kernel-regression.html" rel="bookmark"
                           title="Permalink to Regression_6_Nearest Neighbor and Kernel Regression">Regression_6_Nearest Neighbor and Kernel Regression</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sun 17 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>Previously we have talked about LASSO in which we used norm 1 as the constraints to select features ad get sparse solutions. In this article, we will continue to discuss nearest neighbor and kernel regression.</p>
                <a class="readmore" href="/regression_6_nearest-neighbor-and-kernel-regression.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_5_lasso.html" rel="bookmark"
                           title="Permalink to Regression_5_LASSO">Regression_5_LASSO</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Fri 15 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>Previously we have talked about ridge regression in which we used norm 2 as the constraints to balance the bias and variance. In this article, we will use norm 1 which is called least absolute shrinkage and selection operator and LASSO for short.</p>
                <a class="readmore" href="/regression_5_lasso.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_4_ridge_regression.html" rel="bookmark"
                           title="Permalink to Regression_4_Ridge_Regression">Regression_4_Ridge_Regression</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Fri 08 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>Previously we have talked about simple regression and for a machine learning model we want to get a tradeoff between bias and variance. Now we introduce one tuning parameter and check how ridge regression works.</p>
                <a class="readmore" href="/regression_4_ridge_regression.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_3_simple_regression.html" rel="bookmark"
                           title="Permalink to Regression_3_Simple_Regression">Regression_3_Simple_Regression</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Tue 05 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>We have generic models and algorithms to solve them. But we always have to worry about how much i am losing. If our model prediction is too low, then we only get low offers; if model prediction is too high, we get few looks and no offers. So this article will presents how to assess our models.</p>
                <a class="readmore" href="/regression_3_simple_regression.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_2_multiple_regression.html" rel="bookmark"
                           title="Permalink to Regression_2_Multiple_Regression">Regression_2_Multiple_Regression</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sat 02 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>What is regression? In this article, i start from simple examples and extract out the basic model for regression problems. Then i will continue to talk about practical issues when we use the model to solve real problems.</p>
                <a class="readmore" href="/regression_2_multiple_regression.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_1_simple_regression.html" rel="bookmark"
                           title="Permalink to Regression_1_Simple_Regression">Regression_1_Simple_Regression</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Mon 21 March 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>What is regression? In this article, i start from simple examples and extract out the basic model for regression problems. Then i will continue to talk about practical issues when we use the model to solve real problems.</p>
                <a class="readmore" href="/regression_1_simple_regression.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
            </ol><!-- /#posts-list -->
<p class="paginator">
    Page 1 / 2
        <a href="/category/machine_learning2.html">&raquo;</a>
</p>
            </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://twitter.com/VicWeituo">twitter</a></li>
                            <li><a href="https://github.com/weituo12321">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <p>Powered by <a href="http://getpelican.com/">Pelican</a>. Theme <a href="https://github.com/blueicefield/pelican-blueidea/">blueidea</a>, inspired by the default theme.</p>
        </footer><!-- /#contentinfo -->

</body>
</html>