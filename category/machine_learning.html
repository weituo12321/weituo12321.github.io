<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>HardCoreAI - Machine_Learning</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
<a href="https://github.com/weituo12321">
<img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub" />
</a>
        <header id="banner" class="body">
                <h1><a href="/">HardCoreAI </a></h1>
                <nav><ul>
                    <li><a href="/category/announcement.html">Announcement</a></li>
                    <li class="active"><a href="/category/machine_learning.html">Machine_Learning</a></li>
                    <li><a href="/category/trial.html">trial</a></li>
                </ul>
                </nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/regression_3_simple_regression.html">Regression_3_Simple_Regression</a></h1>
<footer class="post-info">
        <span>Tue 05 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info --><h1>Measuring loss</h1>
<p>We can use \( L(y, f_\hat{w}(x)) \) to represent the loss function. Here we assume it is a symetric loss function, which means the loss for underpredicting equals overpredicting.Let's first take a look at the training error. Here we want to find \( \hat{w} \) to minimize the RSS on training data set. It is shown in the following figure.  </p>
<p><img alt="im7" src="/images/im7.png" />  </p>
<p>Formally we consider the average of loss on training data. It can be described as follows:  </p>
<div class="math">$$\frac{1}{N}\sum_{i=1}^{N}(y_i-f_\hat{w}(x_i))^2$$</div>
<p>where \( (y_i, x_i) \in\) train_data. OK, you are so smart to recognize it is actually RMSE(root mean square error). As we can see that if we draw training_error with respect to model complexity, we will see a decreasing curve. Will this curve eventually touch the horizontal axis as the model complexity increases? The answer is YES!  </p>
<p><img alt="im8" src="/images/im8.png" />  </p>
<p>So does a small training error mean good predictions? The answer should be NO. We can never be so sure since there is one exception: the training error is everything or all the data we get. It seldom happens but it is a corner case. We will come back to this issue later in this article.  </p>
<p>How about generalization error, sometimes called true error? Generalization error seems not familiar with us. Let's say the house size around us usually follows a normal distribution. Similarly, the house price also follows a normal distribution given the house size. Following two figures explain how it looks like.  </p>
<p><img alt="im9" src="/images/im9.png" />
<img alt="im10" src="/images/im10.png" />  </p>
<p>Since we can give a predicted price, and the true price follows a normal distribution. So the generalization error(true error) is the expectation of the loss between predicted price and true price. It means average over all possible (x,y) pairs weighted by how likely each is \( E_{x,y}[L(y, f_\hat{w}(x))]\). Imagine we draw price and house size in a 3D space. We pick out a slice of the column and look at the slice from its side. Then we will see a normal distribution. (I know it's a little tricky to describe it). Forget it and continue to see the generalization error and model complexity.  </p>
<p>If the model is not that complex, it is likely lie around the center of the column. Then the loss is acceptable. If the model becomes more complex, then it may get closer along the wriggling center line. The generalization goes down.However, as the model complexity continue to increase, the fit line will be crazy. It goes beyond the column too much. So the generalization error increases. The following three figures show this process.  </p>
<p><img alt="im11" src="/images/im11.png" />
<img alt="im12" src="/images/im12.png" />
<img alt="im13" src="/images/im13.png" />  </p>
<p>So the curve describe relationship between the generalization error and model complexity looks like a "U"  </p>
<p>How about test error? We can transform the equation used on train data to test data. The curve describing relationship between test error and model complexity looks liks vibration around the generalization error. It is because each set of test data is different from each other. So my teacher gave us a very useful figure to compare train error, true error, test error:  </p>
<p><img alt="im14" src="/images/im14.png" />  </p>
<p>Now we can talk about overfitting. Suppose current model uses parameter \( \hat{w}\). If there exists a model with estimated \( w'\) such that<br />
1. training_error(\(\hat{w}\)) &lt; training_error(\(w'\))<br />
2. true_error(\( \hat{w}\)) &gt; true_error(\( w'\))  </p>
<p>Typically, if we have enough test points, we may get a good generalization error. But if not enough data points for training, we turn to validation or cross-validation for help.  </p>
<h1>Sources of error</h1>
<p>One may ask where these error come from? Can we kill all of them and get the perfect model in the universe? The answer is NO...Actually error comes from three sources: noise, bias and variance. Let's look at them one by one.  </p>
<h4>Noise</h4>
<p>Data is inherently noisy. Remember the concept in our physics class back to high school? We use the best instrument ruler and correct measuring way, we sometimes still get different length of our physics book. So for each data point we have:  </p>
<div class="math">$$y_i=f_{w(true)}(x_i)+\epsilon_i$$</div>
<p>The \( \epsilon_i\) is our old friend:noise. He goes nowhere and is irreducible.  </p>
<h4>Bias</h4>
<p>Bias can be described as the following equation:  </p>
<div class="math">$$Bias(x) = f_{w(true)}(x)-f_{\bar{w}}(x)$$</div>
<p>In particular, we have different sets of data, we train one set and get a \( w\), then we train another then get another \(w \). At last we have a lot of w. So we have an average fit,this average fit is \( f_{\bar{w}}(x)\). Bias is the difference between the average fit and the true fit. It means if our model is flexible enough to include the true fit. The following is an example of simple model  </p>
<p><img alt="im15" src="/images/im15.png" /> </p>
<h4>Variance</h4>
<p>Variance means how much the specific fit varies from the expected fit. We can compare model with low complexity and high complexity in the following two figures.  </p>
<p><img alt="im16" src="/images/im16.png" />
<img alt="im17" src="/images/im17.png" />  </p>
<p>So usually low complexity means low variance, high bias and high complexity means high variance, low bias. If we add or remove data points and refit, the model with high complexity may be very different from current curve since it is so flexible. So what model complexity should we use? We draw variance, bias and their combination on the same figure.  </p>
<p><img alt="im18" src="/images/im18.png" />  </p>
<p>Actually we have \(MSE = bias^2+variance\). It looks like a "U". The bottom of the "U" is our sweet point. Almost all the machine learning problems ask us to find the sweet point. But wait, can we really calculate the MSE value? We can't because we don't know the \( f_{w(true)} \) and can't get all possible sets of data. In a practical way, we still turn to validation or cross-validation method.  </p>
<p>Lastly, we need to see the relationship between error and the amount of data. Given a model with complexity fixed, as the amount of data increases, the training error will increase because with few data points, fixed complexity model can fit those points well, but with more data points, the model gradually loses its control. On the contrast, the generalization error will decrease because with few data points the model can't tell too much about the true curve. However, the generalization error and training error will get closer to each other and the bias plus noise will always be there.  </p>
<h1>Depth on sources of error</h1>
<p>Alright, math time now. Let's take a closer look at the error of our model.  </p>
<div class="math">$$Error=\sigma^2 +[bias(f_{\hat{w}})]^2+var(f_{\hat{w}})$$</div>
<p>We check the expected prediction error on a specific data points \( x_{t}\). It is described as:  </p>
<div class="math">$$L(y, f_{\hat{w}}(x))=(y-f_{\hat{w}}(x_t))^2$$</div>
<p>So the expected prediction error at \( x_t\) is:  </p>
<div class="math">$$E_{train}[(y-f_{\hat{w}}(x_t))^2]$$</div>
<p>Here we will use a commonly used derivation trick called "add zero". The above error can be written as:  </p>
<div class="math">$$E_{train}[((y_t-f_{w(true)}(x_t))+(f_{w(true)}(x_t)-f_{\hat{w}(train)}(x_t)))^2]$$</div>
<div class="math">$$=E[(y-f)^2]+2E[(y-f)(f-\hat{f})+E[(f-\hat{f})^2]]$$</div>
<div class="math">$$=\sigma^2+2E[y-f]E[f-\hat{f}]+MSE(\hat{f})$$</div>
<p>In particular, \( E[y-f]=E[\epsilon]=0\), so finally we have:  </p>
<div class="math">$$E_{train}[(y-f_\hat{w}(x_t))^2]=\sigma^2 + MSE(\hat{f})$$</div>
<p>And  </p>
<div class="math">$$MSE[f_{\hat{w}(true)}(x_t)]=E_{train}[(f-\hat{f})^2]$$</div>
<p>We use the same "add zero" trick to simplify the MSE:</p>
<div class="math">$$E_{train}[(f-\hat{f})^2]=E_{train}[((f-\bar{f})+(\bar{f}-f))^2]$$</div>
<div class="math">$$=E_{train}[(f-\bar{f})^2]+2E_{train}[(f-\bar{f})(\bar{f}-\hat{f})]+E_{train}[(\hat{f}-\bar{f})^2]$$</div>
<div class="math">$$=(f-\bar{f})^2+2(f-\bar{f})E[\bar{f}-\hat{f}]+E_{train}[(\hat{f}-\bar{f})^2]$$</div>
<p>Note that \(f-\bar{f}=bias \), \( \bar{f}-E[\hat{f}]=0\) and also we have \(E_{train}[(\hat{f}-\bar{f})^2]=var(\hat{f}) \) and replace the corresponding term.  </p>
<p>So, what we have now? Surprise~~It is:  </p>
<div class="math">$$Error=\sigma^2+bias^2(\hat{f})+var(\hat{f})$$</div>
<p>That is where three sources error come from~  </p>
<h1>Model selection</h1>
<p>For model selection we need to choose tuning paramters \( \lambda\) to control model complexity(e.g. degree of polynomial). So usually we divide data into train data, validation data, test data. We will never touch test data until we finalize our model. We select \( \lambda^*\) to minimize error on validation set. Then we approximate generalization error by test error. Typical splits follows the percentage like: 80%, 10%,10% or 50%,25%,25% for training, validation, testing set separately.  </p>
<h1>Summary</h1>
<p>So far, we have talked about simple regression, multiple regression and model assessment. Actually we can do a lot of fun stuff using these basic tools now. Since we have introduce some tuning parameters, next we will continue to talk more about tuning parameters and will cover ridge regression, LASSO, and kernel regression. Stay tune~</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_2_multiple_regression.html" rel="bookmark"
                           title="Permalink to Regression_2_Multiple_Regression">Regression_2_Multiple_Regression</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sat 02 April 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>What is regression? In this article, i start from simple examples and extract out the basic model for regression problems. Then i will continue to talk about practical issues when we use the model to solve real problems.</p>
                <a class="readmore" href="/regression_2_multiple_regression.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/regression_1_simple_regression.html" rel="bookmark"
                           title="Permalink to Regression_1_Simple_Regression">Regression_1_Simple_Regression</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Mon 21 March 2016</span>
<span>| tags: <a href="/tag/regression.html">regression</a></span>
</footer><!-- /.post-info -->                <p>What is regression? In this article, i start from simple examples and extract out the basic model for regression problems. Then i will continue to talk about practical issues when we use the model to solve real problems.</p>
                <a class="readmore" href="/regression_1_simple_regression.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/re.html" rel="bookmark"
                           title="Permalink to First-Blog">First-Blog</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <span>Sat 19 March 2016</span>
<span>| tags: <a href="/tag/pelican.html">pelican</a><a href="/tag/publishing.html">publishing</a></span>
</footer><!-- /.post-info -->                <p>Try some format</p>
                <a class="readmore" href="/re.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
            </ol><!-- /#posts-list -->
<p class="paginator">
    Page 1 / 1
</p>
            </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://twitter.com/VicWeituo">twitter</a></li>
                            <li><a href="https://github.com/weituo12321">github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <p>Powered by <a href="http://getpelican.com/">Pelican</a>. Theme <a href="https://github.com/blueicefield/pelican-blueidea/">blueidea</a>, inspired by the default theme.</p>
        </footer><!-- /#contentinfo -->

</body>
</html>